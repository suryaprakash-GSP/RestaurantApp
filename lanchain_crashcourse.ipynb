{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c26845d",
   "metadata": {},
   "source": [
    "# Langchain crash course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret_key import openapi_key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = openapi_key\n",
    "\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = \"add your key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0dc6a",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa352d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"The Spice Bazaar\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56e8581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCurrytopia'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782a2dd",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a306b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")\n",
    "print(p)\n",
    "\n",
    "# print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af406b92",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba65c213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTaco Palacio'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "chain.run(\"Mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ccee75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Mexican food. Suggest a fency name for this.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nCantina Azul'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
    "chain.run(\"Mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21098937",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a98d9f",
   "metadata": {},
   "source": [
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9fd9a79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\G.SURYAPRAKASH\\Desktop\\langchain\\lanchain_crashcourse.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/G.SURYAPRAKASH/Desktop/langchain/lanchain_crashcourse.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m SimpleSequentialChain\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/G.SURYAPRAKASH/Desktop/langchain/lanchain_crashcourse.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m chain \u001b[39m=\u001b[39m SimpleSequentialChain(chains \u001b[39m=\u001b[39m [name_chain, food_items_chain])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/G.SURYAPRAKASH/Desktop/langchain/lanchain_crashcourse.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m content \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mIndian\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/G.SURYAPRAKASH/Desktop/langchain/lanchain_crashcourse.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(content)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:487\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    486\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    488\u001b[0m         _output_key\n\u001b[0;32m    489\u001b[0m     ]\n\u001b[0;32m    491\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[0;32m    493\u001b[0m         _output_key\n\u001b[0;32m    494\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    296\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    281\u001b[0m     inputs,\n\u001b[0;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    289\u001b[0m     )\n\u001b[0;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\sequential.py:179\u001b[0m, in \u001b[0;36mSimpleSequentialChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    177\u001b[0m color_mapping \u001b[39m=\u001b[39m get_color_mapping([\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchains))])\n\u001b[0;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m i, chain \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchains):\n\u001b[1;32m--> 179\u001b[0m     _input \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49mrun(_input, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstep_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrip_outputs:\n\u001b[0;32m    181\u001b[0m         _input \u001b[39m=\u001b[39m _input\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:487\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    486\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    488\u001b[0m         _output_key\n\u001b[0;32m    489\u001b[0m     ]\n\u001b[0;32m    491\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[0;32m    493\u001b[0m         _output_key\n\u001b[0;32m    494\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    296\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    281\u001b[0m     inputs,\n\u001b[0;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    289\u001b[0m     )\n\u001b[0;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m    104\u001b[0m     prompts,\n\u001b[0;32m    105\u001b[0m     stop,\n\u001b[0;32m    106\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[0;32m    108\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:504\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    497\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    498\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    502\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    503\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:653\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    640\u001b[0m         )\n\u001b[0;32m    641\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    642\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    643\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m         )\n\u001b[0;32m    652\u001b[0m     ]\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[0;32m    654\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    655\u001b[0m     )\n\u001b[0;32m    656\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:541\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    540\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 541\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    542\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    543\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\base.py:528\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    520\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    525\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    526\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    527\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 528\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m    529\u001b[0m                 prompts,\n\u001b[0;32m    530\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    531\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    532\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    533\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    534\u001b[0m             )\n\u001b[0;32m    535\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    536\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    537\u001b[0m         )\n\u001b[0;32m    538\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\openai.py:387\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    376\u001b[0m         {\n\u001b[0;32m    377\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m         }\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 387\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[0;32m    388\u001b[0m         \u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    389\u001b[0m     )\n\u001b[0;32m    390\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    391\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\llms\\openai.py:113\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for default-text-davinci-003 in organization org-3b8x1uEeKH5BdudPQiQwnQHl on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "content = chain.run(\"Indian\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386d05c",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49dc0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dea8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec1be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653c540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'restaurant_name': \"\\n\\nMaharaja's Palace Cuisine\",\n",
       " 'menu_items': '\\n\\n-Malai Kofta\\n-Butter Chicken\\n-Lamb Biryani\\n-Tandoori Paneer\\n-Saag Paneer\\n-Palak Paneer\\n-Matar Paneer\\n-Chana Masala\\n-Aloo Gobi\\n-Baingan Bharta\\n-Shahi Paneer\\n-Chole Bhature\\n-Papdi Chaat\\n-Samosa Chaat\\n-Kulfi'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"cuisine\": \"Indian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069a75e",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ee719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure yoy have installed this package: pip install google-search-results\n",
    "# from secret_key import serpapi_key\n",
    "# os.environ['SERPAPI_API_KEY'] = serpapi_key\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = \"add your serpapi key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b2c6b",
   "metadata": {},
   "source": [
    "#### serpapi and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4212d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the GDP of US in 2022\n",
      "Action: Search\n",
      "Action Input: US GDP in 2022\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m$25.46 trillion\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to add 5 to this number\n",
      "Action: Calculator\n",
      "Action Input: 25.46 + 5\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 30.46\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The GDP of US in 2022 plus 5 is $30.46 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The GDP of US in 2022 plus 5 is $30.46 trillion.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd3a12",
   "metadata": {},
   "source": [
    "#### Wikipedia and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d06ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Elon Musk was born and then calculate his age.\n",
      "Action: Wikipedia\n",
      "Action Input: Elon Musk\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Elon Musk\n",
      "Summary: Elon Reeve Musk ( EE-lon; born June 28, 1971) is a business magnate and investor. He is the founder, CEO, and chief engineer of SpaceX; angel investor, CEO, and product architect of Tesla, Inc.; owner, CTO, and chairman of Twitter; founder of the Boring Company and X Corp.; co-founder of Neuralink and OpenAI; and president of the philanthropic Musk Foundation. Musk is the wealthiest person in the world according to the Bloomberg Billionaires Index and Forbes's Real Time Billionaires list as of June 2023, primarily from his ownership stakes in Tesla and SpaceX, with an estimated net worth of around $225 billion according to Bloomberg and $230.2 billion according to Forbes.Musk was born in Pretoria, South Africa, and briefly attended the University of Pretoria before moving to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University and transferred to the University of Pennsylvania, where he received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University. After two days, he dropped out and, with his brother Kimbal, co-founded the online city guide software company Zip2. In 1999, Zip2 was acquired by Compaq for $307 million and Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal, which eBay acquired for $1.5 billion in 2002.\n",
      "With $175.8 million, Musk founded SpaceX in 2002, a spaceflight services company. In 2004, he was an early investor in the electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, he helped create SolarCity, a solar energy company that was later acquired by Tesla and became Tesla Energy. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, he co-founded Neuralinka neurotechnology company developing braincomputer interfacesand the Boring Company, a tunnel construction company. Musk has also proposed a hyperloop high-speed vactrain transportation system. In 2022, his acquisition of Twitter for $44 billion was completed. \n",
      "Musk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including that of spreading COVID-19 misinformation. In 2018, the U.S. Securities and Exchange Commission (SEC) sued Musk for falsely tweeting that he had secured funding for a private takeover of Tesla. Musk stepped down as chairman of Tesla and paid a $20 million fine as part of a settlement agreement with the SEC.\n",
      "\n",
      "\n",
      "\n",
      "Page: Acquisition of Twitter by Elon Musk\n",
      "Summary: Business magnate Elon Musk initiated an acquisition of American social media company Twitter, Inc. on April 14, 2022, and concluded it on October 27, 2022. Musk had begun buying shares of the company in January 2022, becoming its largest shareholder by April with a 9.1 percent ownership stake. Twitter invited Musk to join its board of directors, an offer he initially accepted before declining. On April 14, Musk made an unsolicited offer to purchase the company, to which Twitter's board initially responded with a \"poison pill\" strategy to resist a hostile takeover, before unanimously accepting Musk's buyout offer of $44 billion on April 25. Musk stated that he planned to introduce new features to the platform, make its algorithms open-source, combat spambot accounts, and promote free speech.\n",
      "In July, Musk announced his intention to terminate the agreement, asserting that Twitter had breached their agreement by refusing to crack down on spambot accounts. The company filed a lawsuit against Musk in the Delaware Court of Chancery shortly thereafter, with a trial scheduled for the week of October 17. Weeks before the trial was set to begin, Musk reversed course, announcing that he would move forward with the acquisition. The deal was closed on October 27, with\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Elon Musk was born on June 28, 1971 and is currently 52 years old in 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elon Musk was born on June 28, 1971 and is currently 52 years old in 2023.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"When was Elon musk born? What is his age right now in 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be7ee7",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acab5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Fiesta\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc200f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maharaja's Palace Cuisine\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492fb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871492be",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eea298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Fiesta\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Al-Fez Restaurant\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc88888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "Taco Fiesta\n",
      "Human: Arabic\n",
      "AI: \n",
      "\n",
      "Al-Fez Restaurant\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a88b5b",
   "metadata": {},
   "source": [
    "#### ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ddd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad5062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was won by the West Indies in 1975.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c80b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 10.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07342f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the West Indies team that won the first Cricket World Cup in 1975 was Clive Lloyd.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e459d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI:  The first Cricket World Cup was won by the West Indies in 1975.\n",
      "Human: How much is 5+5?\n",
      "AI:  10.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI:  The captain of the West Indies team that won the first Cricket World Cup in 1975 was Clive Lloyd.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa3abd",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eb33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was held in 1975 and the winner was the West Indies.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395beaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is 10.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b24745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
